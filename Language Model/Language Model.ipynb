{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS918-Exercise 1\n",
    "## Text preprocessing, N-grams and language models\n",
    "### Nan.Liu.1@warwick.ac.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions\n",
    "Function **remove_nonalpha()** is for removing all non-alphanumeric characters except spaces.  \n",
    "Function **remove_character()** is for removing words with only 1 character.  \n",
    "Function **remove_digit()** is for removing numbers that are fully made of digits.  \n",
    "Function **remove_url()** is for removing URLs.  \n",
    "Function **count_opinion_word()** is for counting the number of positive and negative words.  \n",
    "Function **compute_freq_dist()** is for counting the frequency of every n-gram in given n-gram list.  \n",
    "Function **sort_freqdist()** is for sorting the frequency of every ngram and get the top n ngrams based on the number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "\n",
    "# Function - Part A\n",
    "def remove_nonalpha(string):\n",
    "    \"\"\"Function remove_nonalpha() is for removing all non-alphanumeric characters except spaces.\"\"\"\n",
    "    pattern = re.compile('[^a-z0-9 ]+')\n",
    "    newstring = pattern.sub(' ', string)\n",
    "    return newstring\n",
    "\n",
    "\n",
    "def remove_character(string):\n",
    "    \"\"\"Function remove_character() is for removing words with only 1 character.\"\"\"\n",
    "    pattern = re.compile('\\\\b[a-z]\\\\b')\n",
    "    newstring = pattern.sub(' ', string)\n",
    "    newstring = re.sub('[ ]{2,}', ' ', newstring)\n",
    "    return newstring\n",
    "\n",
    "\n",
    "def remove_digit(string):\n",
    "    \"\"\"Function remove_digit() is for removing numbers that are fully made of digits.\"\"\"\n",
    "    pattern = re.compile('\\\\b[0-9]+\\\\b')\n",
    "    newstring = pattern.sub(' ', string)\n",
    "    newstring = re.sub('[ ]{2,}', ' ', newstring)\n",
    "    return newstring\n",
    "\n",
    "\n",
    "def remove_url(string):\n",
    "    \"\"\"Function remove_url() is for removing URLs.\"\"\"\n",
    "    pattern = re.compile('((https?):\\/\\/)?([a-z0-9]+([\\.\\/\\?\\=\\&\\*\\%\\-]+[a-z0-9]+)+)')\n",
    "    newstring = pattern.sub(' ', string)\n",
    "    return newstring\n",
    "\n",
    "\n",
    "# Function - Part B\n",
    "def count_opinion_word(word_list, opinion_dict):\n",
    "    \"\"\"\n",
    "        Function count_opinion_word() is for counting the number of positive and negative words.\n",
    "        input:  word_list is a list needed to count positive and negative words\n",
    "                opinion_dict is a dictionary containing positive and negative words\n",
    "        output: the numbers of negative and positive words in the given word_list\n",
    "    \"\"\"\n",
    "    num_neg = 0\n",
    "    num_pos = 0\n",
    "    for word in word_list:\n",
    "        if word in opinion_dict.keys():\n",
    "            if opinion_dict[word] == 1:\n",
    "                num_pos += 1\n",
    "            else:\n",
    "                num_neg += 1\n",
    "    return num_neg, num_pos\n",
    "\n",
    "\n",
    "def compute_freq_dist(ngrams):\n",
    "    \"\"\"\n",
    "        Function compute_freq_dist() is for counting the frequency of every n-gram in given n-gram list.\n",
    "        input:  ngrams is the return value of nltk.ngrams()\n",
    "        output: freqdict is a dictionary containing n-grams and corresponding frequency\n",
    "    \"\"\"\n",
    "    freqdict = {}\n",
    "    for ngram in ngrams:\n",
    "        if ngram in freqdict.keys():\n",
    "            freqdict[ngram] += 1\n",
    "        else:\n",
    "            freqdict.update({ngram: 1})\n",
    "    return freqdict\n",
    "\n",
    "\n",
    "def sort_freqdist(fdist,n):\n",
    "    \"\"\"\n",
    "        Function sort_freqdist() is for sorting the frequency of every ngram and get the top n ngrams based on the number of occurrences.\n",
    "        input:  fdist is a dictionary containing n-grams and corresponding frequency\n",
    "        output: list of top n ngrams.\n",
    "    \"\"\"\n",
    "    sorted_list = sorted(fdist.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_list[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files\n",
    "Lowercase all the text in signal-news when opening the first file.  \n",
    "Save the positive and negative words as dictionaries for higher efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "t1 = time.time()\n",
    "with open(\"signal-news1/signal-news1.jsonl\", 'r') as f:\n",
    "    content = []\n",
    "    for line in f:\n",
    "        signal = json.loads(line)\n",
    "        content.append(signal['content'].lower())   # Lowercase all the text in signal-news when opening the first file.\n",
    "\n",
    "with open(\"signal-news1/opinion-lexicon-English/negative-words.txt\") as f1, open(\"signal-news1/opinion-lexicon-English/positive-words.txt\") as f2:\n",
    "    neg_word = f1.read().strip().split(\"\\n\")\n",
    "    pos_word = f2.read().strip().split(\"\\n\")\n",
    "\n",
    "\"\"\"Save the positive and negative words as dictionaries for higher efficiency.\"\"\"\n",
    "opinion_dict = {}\n",
    "for word in neg_word:\n",
    "    opinion_dict.update({word: -1})\n",
    "for word in pos_word:\n",
    "    opinion_dict.update({word: 1})\n",
    "# print(opinion_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Text preprocessing  \n",
    "Use the function difined in the beginning remove URLs first, then non-alphanumeric characters, then 1 character and digits.  \n",
    "Use **WordNetLemmatizer().lemmatize()** to lemmatize word one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part A - 1\n",
    "newcontent = []\n",
    "for eachline in content:\n",
    "    eachline = remove_url(eachline)\n",
    "    eachline = remove_nonalpha(eachline)\n",
    "    eachline = remove_character(eachline)\n",
    "    eachline = remove_digit(eachline)\n",
    "    newcontent.append(eachline)\n",
    "# print(newcontent[0])\n",
    "\n",
    "# Part A - 2\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "lemma_content = []\n",
    "for line in newcontent:\n",
    "    line = line.split(' ')\n",
    "    lemma_word = []\n",
    "    for word in line:\n",
    "        lemma_word.append(wnl.lemmatize(word))\n",
    "    lemma_content.append(lemma_word)\n",
    "# print(lemma_content[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: N-grams\n",
    "Compute the number of tokens and vocabulary size  \n",
    "Use **sort_freqdist()** to get the list of top 25 trigrams based on the number of occurrences on the entire corpus.  \n",
    "Use **count_opinion_word()** to compute the number of positive and negative word based on the lists of positive and negative words provided.  \n",
    "Use **count_opinion_word()** to get the number of positive and negative wotd in each news story and then compare them to get the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Part B======\n",
      "The number of tokens is  5641693\n",
      "The vocabulary size is  90177\n",
      "\n",
      "The top 25 trigrams are:\n",
      "(('one', 'of', 'the'), 2438)\n",
      "(('on', 'share', 'of'), 2093)\n",
      "(('on', 'the', 'stock'), 1567)\n",
      "(('a', 'well', 'a'), 1427)\n",
      "(('in', 'research', 'report'), 1417)\n",
      "(('samsung', 'samsung', 'samsung'), 1400)\n",
      "(('in', 'research', 'note'), 1375)\n",
      "(('the', 'united', 'state'), 1226)\n",
      "(('for', 'the', 'quarter'), 1222)\n",
      "(('average', 'price', 'of'), 1193)\n",
      "(('research', 'report', 'on'), 1177)\n",
      "(('research', 'note', 'on'), 1138)\n",
      "(('share', 'of', 'the'), 1133)\n",
      "(('the', 'end', 'of'), 1130)\n",
      "(('in', 'report', 'on'), 1124)\n",
      "(('earnings', 'per', 'share'), 1121)\n",
      "(('cell', 'phone', 'plan'), 1073)\n",
      "(('phone', 'plan', 'detail'), 1070)\n",
      "(('according', 'to', 'the'), 1066)\n",
      "(('of', 'the', 'company'), 1064)\n",
      "(('buy', 'rating', 'to'), 1016)\n",
      "(('appeared', 'first', 'on'), 995)\n",
      "(('moving', 'average', 'price'), 995)\n",
      "(('day', 'moving', 'average'), 993)\n",
      "(('nokia', 'nokia', 'nokia'), 984)\n",
      "\n",
      "The number of positive word counts is:  171496\n",
      "The number of negative word counts is:  129520\n",
      "\n",
      "The number of news stories with more positive words is:  10906\n",
      "The number of news stories with more negative words is:  6357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part B - 1\n",
    "print(\"======Part B======\")\n",
    "all_word = []\n",
    "for line in lemma_content:\n",
    "    for word in line:\n",
    "        all_word.append(word)\n",
    "\n",
    "N = len(all_word)\n",
    "V = len(set(all_word))\n",
    "print('The number of tokens is: ', N)\n",
    "print('The vocabulary size is: ', V)\n",
    "print()\n",
    "\n",
    "# Part B - 2\n",
    "trigram = nltk.ngrams(all_word, 3)\n",
    "fdist = compute_freq_dist(trigram)\n",
    "top_25 = sort_freqdist(fdist, 25)\n",
    "\n",
    "print(\"The top 25 trigrams are:\")\n",
    "for f in top_25:\n",
    "    print(f)\n",
    "print()\n",
    "\n",
    "# Part B - 3\n",
    "num_neg, num_pos = count_opinion_word(all_word, opinion_dict)\n",
    "print(\"The number of positive word counts is: \", num_pos)\n",
    "print(\"The number of negative word counts is: \", num_neg)\n",
    "print()\n",
    "\n",
    "# Part B - 4\n",
    "pos_news = 0\n",
    "neg_news = 0\n",
    "for eachline in lemma_content:\n",
    "    num_neg, num_pos = count_opinion_word(eachline, opinion_dict)\n",
    "    if num_pos > num_neg:\n",
    "        pos_news += 1\n",
    "    elif num_pos < num_neg:\n",
    "        neg_news += 1\n",
    "print(\"The number of news stories with more positive words is: \", pos_news)\n",
    "print(\"The number of news stories with more negative words is: \", neg_news)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Language models \n",
    "Use **nltk.ngrams(), compute_freq_dist()** to compute language models for trigrams based on the first 16,000 rows.  \n",
    "Use **sort_freqdist()** to get the dictionary in descending order of the frequency and produce a sentence of 10 words beginning with the bigram \"is this\".  \n",
    "Use Laplace Smoothing to deal with new words and log transformation to get the perplexity by evaluating on the remaining rows of the corpus(rows 16,001+)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Part C======\n",
      "the sentence beginning with \"is this\":\n",
      " is this the company ha market capitalization of billion and\n",
      "\n",
      "The perplexity of the remaining rows is:  12.749818353544384\n",
      "\n",
      "This program takes  143.17313289642334 seconds\n"
     ]
    }
   ],
   "source": [
    "# Part C - 1\n",
    "print(\"======Part C======\")\n",
    "word_train = []\n",
    "for line in lemma_content[:16000]:\n",
    "    for word in line:\n",
    "        word_train.append(word)\n",
    "trigram_train = nltk.ngrams(word_train, 3)\n",
    "trigram_dist = compute_freq_dist(trigram_train)\n",
    "trigram_dict = sort_freqdist(trigram_dist, len(trigram_dist))\n",
    "bigram_train = nltk.ngrams(word_train, 2)\n",
    "bigram_dist = compute_freq_dist(bigram_train)\n",
    "\n",
    "\n",
    "# print(trigram_dist[0])\n",
    "sentence = ['is', 'this']\n",
    "i = 0\n",
    "\"\"\"\n",
    "    Find the most common trigram begin with the given bigram, \n",
    "    append the third word into the sentence. \n",
    "\"\"\"\n",
    "while i < 8:\n",
    "    for j in range(len(trigram_dict)):\n",
    "        if sentence[i] == trigram_dict[j][0][0] and sentence[i+1] == trigram_dict[j][0][1]:\n",
    "            sentence.append(trigram_dict[j][0][2])\n",
    "            i += 1\n",
    "            break\n",
    "print('the sentence beginning with \"is this\":\\n', ' '.join(sentence))\n",
    "print()\n",
    "\n",
    "# Part C - 2\n",
    "tri_test = []\n",
    "for line in lemma_content[16000:]:\n",
    "    word_test = []\n",
    "    for word in line:\n",
    "        word_test.append(word)\n",
    "    tri_test.extend(list(nltk.ngrams(word_test, 3)))\n",
    "# print(tri_test[0])\n",
    "\n",
    "\"\"\"\n",
    "    Compute the perplexity of remaining rows based on the language models from the first 16000 rows.\n",
    "    Use Laplace Smoothing to deal with new words.\n",
    "    Use log transformation to make the perplexity readable.\n",
    "\"\"\"\n",
    "n = len(tri_test)\n",
    "pp_1 = 0\n",
    "for tri in tri_test:\n",
    "    c1 = trigram_dist.get(tri, 0)\n",
    "    c2 = bigram_dist.get((tri[0], tri[1]), 0)\n",
    "    pp_1 += math.log((c2 + n) / (c1 + 1))\n",
    "pp = pp_1 / n\n",
    "print(\"The perplexity of the remaining rows is: \", pp)\n",
    "print()\n",
    "\n",
    "print(\"This program takes \", time.time() - t1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
